{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Master Thesis Presentation Notebook\n",
    "## by ThÃ©o Stassen\n",
    "\n",
    "This notebook presents the test protocol implemented for the purpose of the Master Thesis. \n",
    "The goal is the comparison of different probabilistic forecasting models, mainly from the deep learning field, \n",
    "in the context of the prediction of wind turbines production.\n",
    "\n",
    "The code uses GluonTS, a toolkit which contains components and tools for building time series models using MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Concerning the installation, to run of the code, you need to be installed \n",
    "(see https://gluon-ts.mxnet.io/install.html) :\n",
    "\n",
    "- mxnet, using pip, in a compatible version with gluon ts (>=1.3.1,< 1.5.*, cpu only or with gpu support)  \n",
    "- If you want gpu support, you need a compatible Nvidia gpu, and a compatible version (8.0, 9.0, or 9.2) of \n",
    "CUDA (check if working properly)\n",
    "- gluonts, using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataset as dt # Custom file containing class definition of Dataset\n",
    "\n",
    "# In case of this error : \n",
    "# OSError: libcudart.so.9.2: cannot open shared object file: No such file or directory \n",
    "# if you are on (ubuntu) linux, do : sudo ldconfig /usr/local/cuda/lib64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset Import\n",
    "\n",
    "The project need some entry data. In this version, the input files are in /datasets.\n",
    "They  are array of values. GluonTS needs, to create datasets, some information .\n",
    "   In general, the datasets defined by GluonTS are objects that consists of three main members:\n",
    "\n",
    "- `dataset.train` is an iterable collection of data entries used for training. Each entry corresponds to one time series\n",
    "- `dataset.test` is an iterable collection of data entries used for inference. The test dataset is an extended version \n",
    "of the train dataset that contains a window in the end of each time series that was not seen during training. \n",
    "This window has length equal to the prediction length.\n",
    "- `dataset.metadata` contains metadata of the dataset such as the frequency of the time series, the context (training) length\n",
    "a prediction length, associated features (if we want to combine the temporal series information with other data), etc.\n",
    "\n",
    "\n",
    "  All the models will use the data by splitting the entries (in a way that we can precise) into pieces of size \n",
    "  learning (\"context\") + predict length on which they will train and predict.\n",
    "  \n",
    "  In this first version, only one array is imported and all the time series corresponds to the same source. \n",
    "  Precisely, we import the `6months-minutes.csv` file. The first idea was to use it as a single time serie. The problem \n",
    "  was that the models had very long training time, about 120s per epochs (100 epochs recommended). The data was correctly \n",
    "  splitted but Mxnet is not optimised for this situation. The solution is to split manually the 6 month into \n",
    "  different time series. The natural choice is to split into days. We have so 183 samples/time series of length 1440 time steps\n",
    "  with a frequency of 1 minute. The values are also divided by 1000 because it was causing loss problems.\n",
    "  In future version We could import and combine the data of multiple sources as a single training/testing dataset, \n",
    "  or use multiple sources for training and another for testing.\n",
    "  \n",
    "  The goal is to predict the production in a window of 10 minutes, which means 10 time steps.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/6months-minutes.csv\")\n",
    "imported_dataset = np.array([df['Active Power'].to_numpy()])\n",
    "n_sample = 183\n",
    "imported_dataset = imported_dataset.reshape(n_sample, -1)  # Split by days\n",
    "imported_dataset = imported_dataset / 1000\n",
    "prediction_length = 10\n",
    "context_length = 60 * 1  # One day\n",
    "freq = \"1min\"\n",
    "start = pd.Timestamp(\"01-04-2019\", freq=freq)\n",
    "dataset = dt.Dataset(custom_dataset=imported_dataset, start=start, freq=freq,\n",
    "                     prediction_length=prediction_length,\n",
    "                     learning_length=context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Quantiles\n",
    "\n",
    "The project is to do probabilistic forecasting of wind turbines time series but the goal is not really\n",
    "to minimize the difference (in any ways to consider it) between the predicted values and the observed values.\n",
    "The goal is to obtain a distribution of probability of the future values which have the more correct \"safety\" quantile. \n",
    "What is forbidden is that the model tells us a understimated value of the quantile ( a too thin values window).\n",
    "We want a model which tells us that there are 99% (or another security value) chance that the real value of production \n",
    "will be in this window and that affirmation must be exact (or overestimating) not underestimating.\n",
    "This goal must be combine with the goal to have the thinner estimation window. \n",
    "We want a balance between the not understemation of the risk and the \"precision\" of the predict.\n",
    "\n",
    "The quantile list contains the values of quantiles that are useful for us. We want a 99% precision so we consider the \n",
    "[0.005, 0.995] window and we compute also the 90% and 50% precision to have a gradual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quantiles = list([0.005, 0.05, 0.25, 0.5, 0.75, 0.95, 0.995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics\n",
    "\n",
    "To observe if the models do well what we ask to do, to observe their quality considering the goals that are fixed, we \n",
    "need to choose the appropriate metrics. GluonTS proposes some metrics to evaluate models (MSE, MSIS, etc).\n",
    "\n",
    "Concerning the correctness of the quantiles, none of the GluonTS metric is satisfying for this purpose. The proposed \n",
    "metric, \"Coverage\" is a simple combination of two of them. For a security limit of k :  \n",
    "\n",
    "$Coverage = (k - coverage(k)) - (coverage(1-k) - (1-k)) $\n",
    "\n",
    "With coverage(x) is a GluonTS implemented metric which give the observed probability to have an value below the quantile x.\n",
    "If the distribution is perfect the coverage must tends to the value x. For the upper bound, if coverage(x) > x, the quantile is overlarge and\n",
    "if coverage(x) < x the quantile is underlarge. And it is the opposite for the lower bound.\n",
    "For a 99% precision the security limit is 0.995. \n",
    "\n",
    "$Coverage = (0.995 - coverage(0.995)) - (coverage(0.005) - 0.005) $\n",
    "\n",
    "To observe if the distribution is large, we compute a custom metric which is simply the mean of the difference between \n",
    "the quantile(k) and quantile(1-k). For a time serie :\n",
    "\n",
    "$$Bandwidth = \\frac{1}{p}  \\sum_{i=0}^{p} quantile_{i}(k) - quantile_{i}(1-k) $$\n",
    "\n",
    "Where p is the number of time step and $$quantile_{i}$$ the value of the quantile at time step i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_metric = \"Coverage\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Custom loss\n",
    "\n",
    "As explained in the previous sections, we want the model to give precise estimation of the quantiles.\n",
    "The different models presented in the following sections will give more or less good results but we can interrogate how \n",
    "the model consider that it is good and if we want to modify it.\n",
    "In most of the models, the loss function is the inverse of the log-density of the output distribution.\n",
    "If $ \\mu $ and $ \\sigma $ are the parameters of the distribution output (in the case of a gaussian) by the model : \n",
    "\n",
    "$$ base\\_loss(x) = - ln \\varphi_{\\mu,\\sigma^2}(x)  $$\n",
    "\n",
    "This loss is the natural way to formulate that we want that the distribution corresponds to the observed x.\n",
    "But with this loss, the variation of the loss is inversely proportinal to the deviation between x and the max\n",
    "of the distribution. Between to big deviation, for example if x at the position of the quantile(0.9) and if x is at \n",
    "quantile(0.99), the loss is near.\n",
    "We need another loss with the particular goal to discourage the values of x bigger than quantile(0.99) and lower than \n",
    "quantile(0.01). \n",
    "The proposed loss is the following :\n",
    "\n",
    "$$ alt\\_loss(x) = e^{x - quantile(0.995)} + e^{quantile(0.005)-x}$$\n",
    "\n",
    "The loss increase exponentially if x is outside the security window, is lower and not exponential in the window.\n",
    "\n",
    "In the pre-implemented GluonTs models the only usable loss is $base\\_loss$ but if we create custom models \n",
    "it is possible to modify it. \n",
    "At this time, three custom models (one original, \"Simple\", and the copy of the pre-implemented \"SimpleFeedForward\" and \n",
    "\"CanonicalRNN\" are implemented. And they uses this loss :\n",
    "\n",
    "$$ custom\\_loss = base\\_loss + \\alpha * alt\\_loss $$\n",
    "\n",
    "With $\\alpha$ a parameter that we will make vary when comparing all the models together.\n",
    "Depending of it value the results will be very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.7, 0.8, 0.9]\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Distribution\n",
    "\n",
    "GluonTs proposes different type of output distribution. In most of the models the output is a vector of values which are\n",
    "transformed to a vector of distribution parameters and put in a distribution object.\n",
    "We can precise what kind of distribution.\n",
    "Three of them are working for now. Gaussian, Laplace and PiecewiseLinear.\n",
    "Also implemented there are also Student, but it is rejected because we cannot obtain directly the quantiles of the distribution, \n",
    "that we need for the loss, and Uniform but it make the loss outbound.\n",
    "And there are others, more complicated, like the multiple kernel gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# distributions = [\"Gaussian\", \"Laplace\", \"PiecewiseLinear\", ]\n",
    "# No quantiles in Student, Uniform has a problem with loss\n",
    "\n",
    "distributions = [\"Gaussian\"]\n",
    "distribution = \"Gaussian\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Different models\n",
    "\n",
    "All the deep learning models are presented here. In each section we compare the effect of the main parameters on the\n",
    "coverage and bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from comparison import compare_all_models, compare_simple, compare_simple_feed_forward, compare_canonicalrnn, \\\n",
    "    compare_deepar, compare_deepfactor, compare_mqcnn, compare_mqrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Example of resulting plots\n",
    "\n",
    "We can see what some of the models give as forecast results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "compare_all_models(dataset,\"Gaussian\",0.9,[\"cSimple\", \"cSimpleFeedForward\",\"cCanonicalRNN\"],chosen_metric,epochs,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global comparison\n",
    "\n",
    "Here is the comparison of all models, for different alphas and distribution output type\n",
    " with all the presented in addition of two models not using deep learning\n",
    "(ETS and seasonal naive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "models = [\"cCanonicalRNN\"]\n",
    "compare_all_models(dataset, distributions, alphas, models, chosen_metric, epochs,False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simple\n",
    "\n",
    "A model which uses a simple 2 fully connected layers neural network.\n",
    "The tunable parameter is the number of cells in layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "num_cells = [10, 50, 100, 200, 300]\n",
    "compare_simple(dataset, distribution, alpha, chosen_metric, epochs, num_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## FeedForward\n",
    "\n",
    "A simple MLP model.\n",
    "The tunable parameter is the number of hidden nodes in each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_hidden_dimensions = [[10], [40], [40, 40], [40, 40, 40]]\n",
    "compare_simple_feed_forward(dataset, distribution, alpha, chosen_metric, epochs, num_hidden_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "A model which uses a recurrent neural network.\n",
    "The tunable parameters are the number of layers and number of cells of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Other parameters : num_cells and embedding dimensions\n",
    "num_layers = [1, 2, 5, 10]\n",
    "num_cells = [20,50,100]\n",
    "compare_canonicalrnn(dataset, distribution, alpha, chosen_metric, epochs, \"n_layers\", num_layers)\n",
    "compare_canonicalrnn(dataset, distribution, alpha, chosen_metric, epochs, \"n_cells\", num_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Ar\n",
    "\n",
    "Implementation of DeepAr estimator, a RNN based model, close to the one described in paper \n",
    "[\"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\" ](https://arxiv.org/abs/1704.04110)\n",
    "The tunable parameters are the same that for CanonicalRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_layers = [1, 2, 5, 10]\n",
    "num_cells = [20,40,50,100]\n",
    "compare_deepar(dataset,distribution,alpha,chosen_metric,epochs,\"n_layers\",num_layers)\n",
    "compare_deepar(dataset,distribution,alpha,chosen_metric,epochs,\"n_cells\",num_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Factors\n",
    "\n",
    "Implementation of the 2019 ICML paper [âDeep Factors for Forecastingâ](https://arxiv.org/abs/1905.12417)\n",
    "It uses a global RNN model to learn patterns across multiple related time series and an arbitrary local model to model the time series on a per time series basis. \n",
    "In the current implementation, the local model is a RNN (DF-RNN).\n",
    "The tunable parameters are the number of units per hidden layers and the number of layers in the global model\n",
    ", the number of global factors. We could also add the number of units and layers in the local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_hidden_global = [20,50,100]\n",
    "num_layers_global = [1, 2, 5]\n",
    "num_factors = [5, 10, 20]\n",
    "compare_deepfactor(dataset,distribution,alpha,chosen_metric,epochs,\"n_hidden_global\",num_hidden_global)\n",
    "compare_deepfactor(dataset,distribution,alpha,chosen_metric,epochs,\"n_layers_global\",num_layers_global)\n",
    "compare_deepfactor(dataset,distribution,alpha,chosen_metric,epochs,\"n_factors\",num_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gaussian Process\n",
    "\n",
    "Model using Gaussian Processes (GP).\n",
    "Each time series has a GP.\n",
    "There are no parameters easy to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NPTS\n",
    "\n",
    "Implementation of the Non-Parametric Time Series Forecaster, \n",
    "which falls into the class of simple forecasters that use one of the past observed targets as the forecast for \n",
    "the current time step. It randomly samples a past time index as the prediction for the time step T (it is \n",
    "an auto regressive model which predict time steps one by one). \n",
    "There are no parameters to tune, but some variant are possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MQCNN\n",
    "\n",
    "Discriminative Sequence to Sequence model made using the SeqtoSeq framework of GluonTS to reproduce the model of the \n",
    "paper [A Multi-Horizon Quantile Recurrent Forecaster](https://arxiv.org/abs/1711.11053).\n",
    "Sequence to sequence models are composed of two parts. The encoder network, that reads in a certain context of the training range of the time series \n",
    "and encodes information about  the  sequence  in  a  latent  state.\n",
    "And the decoder network, which generates the forecast by combining the latent information with the features in the prediction range\n",
    "In MQCNN the encoder is a Convolutionnal Neural Network and the decoder an mlp model. The output\n",
    "is not technically a  distribution but the quantiles itself, that are predicted values obtain by optimising\n",
    "the corresponding quantile loss (In general in a deterministic world if you train a model with as loss the quantile\n",
    "loss [0.9], you will obtain the line representing the quantile [0.9])\n",
    "The tunable parameter is the  dimension of the mlp decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp_final_dim = [10,20,30]\n",
    "compare_mqcnn(dataset, distribution, alpha, chosen_metric, epochs, mlp_final_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MQRNN\n",
    "\n",
    "Same as MQCNN but with a Recurrent Neural Network encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp_final_dim = [10,20,30]\n",
    "compare_mqrnn(dataset, distribution, alpha, chosen_metric, epochs, mlp_final_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Other plots implemented\n",
    "\n",
    "## metrics by time series\n",
    "\n",
    "We can obtain the item metrics, i.e. the metrics for each time series instead of the global metrics for all time\n",
    "series. These metrics can be plot as histogramm of metric values, to see the repartition of values in the 183 time\n",
    "series. This works not for all metrics, for coverage the results are not precise. An example is the MSE for the \n",
    "three first models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%md\n"
    }
   },
   "outputs": [],
   "source": [
    "from plots import plot_distr_params, hist_plot_item_metrics\n",
    "compare_all_models(dataset, distributions, alphas, models, \"MSE\", epochs, False)\n",
    "hist_plot_item_metrics(\"MSE\", [\"cSimple\", \"cSimpleFeedForward\",\"cCanonicalRNN\"])  \n",
    "# Not precise for Coverage (too much around)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## distribution information\n",
    "\n",
    "In the custom models we save the exact parameters of output distribution. We can plot the values here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_distr_params([\"cSimple\", \"cSimpleFeedForward\",\"cCanonicalRNN\"], alphas, distributions) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}